import os
import argparse
from pathlib import Path
from typing import List, Dict, Any, Set
from langchain_community.document_loaders import UnstructuredPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ç³»ç»Ÿé…ç½®
CHROMA_DIR = "chroma_db_hardware"

# æ¨¡å‹é…ç½®
EMBEDDING_MODEL = "text-embedding-ada-002"  # OpenAI åµŒå…¥æ¨¡å‹
CHUNK_SIZE = 500  # æ–‡æ¡£åˆ†å—å¤§å°
CHUNK_OVERLAP = 50  # åˆ†å—é‡å å¤§å°

def get_pdf_files(data_dir: str) -> List[str]:
    """è·å–æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰PDFæ–‡ä»¶
    
    Args:
        data_dir: PDFæ–‡ä»¶æ‰€åœ¨ç›®å½•
        
    Returns:
        PDFæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    data_path = Path(data_dir)
    if not data_path.exists():
        raise ValueError(f"ç›®å½•ä¸å­˜åœ¨: {data_dir}")
    
    pdf_files = []
    for file in data_path.glob("**/*.pdf"):
        pdf_files.append(str(file))
    
    if not pdf_files:
        raise ValueError(f"ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶: {data_dir}")
        
    return pdf_files

def get_processed_files(chroma_dir: str) -> Set[str]:
    """è·å–å·²ç»å¤„ç†è¿‡çš„æ–‡ä»¶åˆ—è¡¨
    
    Args:
        chroma_dir: Chromaæ•°æ®åº“ç›®å½•
        
    Returns:
        å·²å¤„ç†æ–‡ä»¶è·¯å¾„é›†åˆ
    """
    if not Path(chroma_dir).exists():
        return set()
        
    embeddings = OpenAIEmbeddings(
        model=EMBEDDING_MODEL,
        show_progress_bar=True
    )
    
    db = Chroma(
        persist_directory=chroma_dir,
        embedding_function=embeddings
    )
    
    # ä»å…ƒæ•°æ®ä¸­è·å–å·²å¤„ç†çš„æ–‡ä»¶è·¯å¾„
    processed = set()
    for doc in db.get()["metadatas"]:
        if doc and "source" in doc:
            processed.add(doc["source"])
    
    return processed

def load_and_process_documents(pdf_paths: List[str]) -> List[Any]:
    """åŠ è½½å¹¶å¤„ç†PDFæ–‡æ¡£
    
    Args:
        pdf_paths: PDFæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        
    Returns:
        åŠ è½½çš„æ–‡æ¡£åˆ—è¡¨
    """
    all_docs = []
    for path in pdf_paths:
        try:
            loader = UnstructuredPDFLoader(path)
            docs = loader.load()
            print(f"âœ… æˆåŠŸåŠ è½½æ–‡æ¡£: {path}")
            all_docs.extend(docs)
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥ {path}: {str(e)}")
    return all_docs

def split_documents(documents: List[Any]) -> List[Any]:
    """åˆ†å‰²æ–‡æ¡£ä¸ºè¾ƒå°çš„å—
    
    Args:
        documents: è¦åˆ†å‰²çš„æ–‡æ¡£åˆ—è¡¨
        
    Returns:
        åˆ†å‰²åçš„æ–‡æ¡£å—åˆ—è¡¨
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?"]  # ä¼˜åŒ–ä¸­æ–‡åˆ†å‰²
    )
    return splitter.split_documents(documents)

def create_or_update_vectorstore(data_dir: str) -> None:
    """åˆ›å»ºæˆ–æ›´æ–°å‘é‡å­˜å‚¨
    
    ä½¿ç”¨ OpenAI çš„ text-embedding-ada-002 æ¨¡å‹ç”Ÿæˆæ–‡æ¡£åµŒå…¥ã€‚
    å¦‚æœå‘é‡å­˜å‚¨å·²å­˜åœ¨ï¼Œåªå¤„ç†æ–°å¢çš„æ–‡æ¡£ã€‚
    
    Args:
        data_dir: PDFæ–‡ä»¶æ‰€åœ¨ç›®å½•
    """
    # è·å–æ‰€æœ‰PDFæ–‡ä»¶
    pdf_files = get_pdf_files(data_dir)
    print(f"ğŸ“ å‘ç° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
    
    # è·å–å·²å¤„ç†çš„æ–‡ä»¶
    processed_files = get_processed_files(CHROMA_DIR)
    print(f"ğŸ’¾ å·²å¤„ç† {len(processed_files)} ä¸ªæ–‡ä»¶")
    
    # æ‰¾å‡ºæ–°å¢çš„æ–‡ä»¶
    new_files = [f for f in pdf_files if f not in processed_files]
    if not new_files:
        print("âœ¨ æ²¡æœ‰æ–°çš„æ–‡ä»¶éœ€è¦å¤„ç†")
        return
    
    print(f"ğŸ†• å‘ç° {len(new_files)} ä¸ªæ–°æ–‡ä»¶")
    
    # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    embeddings = OpenAIEmbeddings(
        model=EMBEDDING_MODEL,
        show_progress_bar=True
    )
    
    # å¤„ç†æ–°æ–‡ä»¶
    documents = load_and_process_documents(new_files)
    chunks = split_documents(documents)
    
    # åˆ›å»ºæˆ–æ›´æ–°å‘é‡å­˜å‚¨
    if Path(CHROMA_DIR).exists():
        print("ğŸ”„ æ›´æ–°ç°æœ‰å‘é‡æ•°æ®åº“")
        db = Chroma(
            persist_directory=CHROMA_DIR,
            embedding_function=embeddings
        )
        db.add_documents(chunks)
    else:
        print("ğŸ†• åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“")
        Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            persist_directory=CHROMA_DIR
        )
    
    print(f"âœ… å¤„ç†å®Œæˆï¼Œæ–°å¢ {len(chunks)} ä¸ªæ–‡æ¡£å—")

def parse_args() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="åˆ›å»ºæˆ–æ›´æ–°æ–‡æ¡£å‘é‡æ•°æ®åº“")
    parser.add_argument(
        "--datadir",
        type=str,
        default="data",
        help="PDFæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼ˆé»˜è®¤ï¼š./dataï¼‰"
    )
    return parser.parse_args()

if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    
    # æ£€æŸ¥OpenAI APIå¯†é’¥
    if not os.getenv("OPENAI_API_KEY"):
        api_key = input("è¯·è¾“å…¥æ‚¨çš„OpenAI APIå¯†é’¥: ").strip()
        os.environ["OPENAI_API_KEY"] = api_key
    
    try:
        create_or_update_vectorstore(args.datadir)
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿé”™è¯¯: {str(e)}") 