import os
import argparse
from pathlib import Path
from typing import List, Dict, Any, Set, Union
from langchain_unstructured import UnstructuredLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_core.embeddings import Embeddings
from langchain_chroma import Chroma
from dotenv import load_dotenv
from embeddings import ZhipuAIEmbeddings

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ç³»ç»Ÿé…ç½®
DEFAULT_CHROMA_DIR = "chroma_db_openai"  # é»˜è®¤Chromaæ•°æ®åº“ç›®å½•

# æ¨¡å‹é…ç½®
OPENAI_MODEL = "text-embedding-ada-002"  # OpenAI åµŒå…¥æ¨¡å‹
ZHIPUAI_MODEL = "embedding-3"  # æ™ºè°±AI åµŒå…¥æ¨¡å‹
CHUNK_SIZE = 500  # æ–‡æ¡£åˆ†å—å¤§å°
CHUNK_OVERLAP = 50  # åˆ†å—é‡å å¤§å°

SUPPORTED_EXTENSIONS = [".pdf", ".docx", ".txt", ".md", ".csv", ".xls", ".xlsx", ".ppt", ".pptx"]

def get_embeddings(model_provider: str) -> Embeddings:
    """è·å–æŒ‡å®šæä¾›å•†çš„åµŒå…¥æ¨¡å‹
    
    Args:
        model_provider: æ¨¡å‹æä¾›å•†ï¼Œå¯é€‰å€¼ï¼š'openai' æˆ– 'zhipuai'
        
    Returns:
        åµŒå…¥æ¨¡å‹å®ä¾‹
        
    Raises:
        ValueError: å½“æä¾›å•†åç§°æ— æ•ˆæ—¶
    """
    if model_provider == "openai":
        if not os.getenv("OPENAI_API_KEY"):
            api_key = input("è¯·è¾“å…¥æ‚¨çš„OpenAI APIå¯†é’¥: ").strip()
            os.environ["OPENAI_API_KEY"] = api_key
        return OpenAIEmbeddings(
            model=OPENAI_MODEL,
            show_progress_bar=True
        )
    elif model_provider == "zhipuai":
        if not os.getenv("ZHIPUAI_API_KEY"):
            api_key = input("è¯·è¾“å…¥æ‚¨çš„æ™ºè°±AI APIå¯†é’¥: ").strip()
            os.environ["ZHIPUAI_API_KEY"] = api_key
        return ZhipuAIEmbeddings(
            model=ZHIPUAI_MODEL,
            dimensions=2048  # æ˜¾å¼æŒ‡å®š2048ç»´åº¦
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: {model_provider}")

def get_supported_files(data_dir: str) -> List[str]:
    """è·å–æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
    
    Args:
        data_dir: PDFæ–‡ä»¶æ‰€åœ¨ç›®å½•
        
    Returns:
        PDFæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    data_path = Path(data_dir)
    if not data_path.exists():
        raise ValueError(f"ç›®å½•ä¸å­˜åœ¨: {data_dir}")
    
    supported_files = []
    for file in data_path.glob("**/*"):
        if file.suffix in SUPPORTED_EXTENSIONS:
            supported_files.append(str(file))
    
    if not supported_files:
        raise ValueError(f"ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ–‡ä»¶: {data_dir}")
        
    return supported_files

def get_processed_files(chroma_dir: str) -> Set[str]:
    """è·å–å·²ç»å¤„ç†è¿‡çš„æ–‡ä»¶åˆ—è¡¨
    
    Args:
        chroma_dir: Chromaæ•°æ®åº“ç›®å½•
        
    Returns:
        å·²å¤„ç†æ–‡ä»¶è·¯å¾„é›†åˆ
    """
    if not Path(chroma_dir).exists():
        return set()
        
    db = Chroma(
        persist_directory=chroma_dir
    )
    
    # ä»å…ƒæ•°æ®ä¸­è·å–å·²å¤„ç†çš„æ–‡ä»¶è·¯å¾„
    processed = set()
    for doc in db.get()["metadatas"]:
        if doc and "source" in doc:
            processed.add(doc["source"])
    
    return processed

def load_and_process_documents(file_paths: List[str]) -> List[Any]:
    """åŠ è½½å¹¶å¤„ç†PDFæ–‡æ¡£
    
    Args:
        pdf_paths: PDFæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        
    Returns:
        åŠ è½½çš„æ–‡æ¡£åˆ—è¡¨
    """
    all_docs = []
    for path in file_paths:
        try:
            print(f"ğŸ” åŠ è½½æ–‡æ¡£: {path}")
            loader = UnstructuredLoader(path)
            docs = loader.load()
            #print(f"ğŸ” åŠ è½½æ–‡æ¡£: {docs}") # æ‰“å°æ–‡æ¡£å†…å®¹
            print(f"âœ… æˆåŠŸåŠ è½½æ–‡æ¡£: {path}")
            all_docs.extend(docs)
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥ {path}: {str(e)}")
    return all_docs

def split_documents(documents: List[Any]) -> List[Any]:
    """åˆ†å‰²æ–‡æ¡£ä¸ºè¾ƒå°çš„å—
    
    Args:
        documents: è¦åˆ†å‰²çš„æ–‡æ¡£åˆ—è¡¨
        
    Returns:
        åˆ†å‰²åçš„æ–‡æ¡£å—åˆ—è¡¨
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?"]  # ä¼˜åŒ–ä¸­æ–‡åˆ†å‰²
    )
    return splitter.split_documents(documents)

def create_or_update_vectorstore(data_dir: str, model_provider: str = "openai", chroma_dir: str = DEFAULT_CHROMA_DIR) -> None:
    """åˆ›å»ºæˆ–æ›´æ–°å‘é‡å­˜å‚¨
    
    ä½¿ç”¨æŒ‡å®šçš„åµŒå…¥æ¨¡å‹ç”Ÿæˆæ–‡æ¡£åµŒå…¥ã€‚
    å¦‚æœå‘é‡å­˜å‚¨å·²å­˜åœ¨ï¼Œåªå¤„ç†æ–°å¢çš„æ–‡æ¡£ã€‚
    
    Args:
        data_dir: PDFæ–‡ä»¶æ‰€åœ¨ç›®å½•
        model_provider: æ¨¡å‹æä¾›å•†ï¼Œå¯é€‰å€¼ï¼š'openai' æˆ– 'zhipuai'
        chroma_dir: Chromaæ•°æ®åº“ç›®å½•è·¯å¾„
    """
    # è·å–æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
    supported_files = get_supported_files(data_dir)
    print(f"ğŸ“ å‘ç° {len(supported_files)} ä¸ªæ”¯æŒçš„æ–‡ä»¶")
    
    # è·å–å·²å¤„ç†çš„æ–‡ä»¶
    processed_files = get_processed_files(chroma_dir)
    print(f"ğŸ’¾ å·²å¤„ç† {len(processed_files)} ä¸ªæ–‡ä»¶")
    
    # æ‰¾å‡ºæ–°å¢çš„æ–‡ä»¶
    new_files = [f for f in supported_files if f not in processed_files]
    if not new_files:
        print("âœ¨ æ²¡æœ‰æ–°çš„æ–‡ä»¶éœ€è¦å¤„ç†")
        return
    
    print(f"ğŸ†• å‘ç° {len(new_files)} ä¸ªæ–°æ–‡ä»¶")
    
    # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    embeddings = get_embeddings(model_provider)
    
    # å¤„ç†æ–°æ–‡ä»¶
    documents = load_and_process_documents(new_files)
    chunks = split_documents(documents)
    
    # åˆ›å»ºæˆ–æ›´æ–°å‘é‡å­˜å‚¨
    if Path(chroma_dir).exists():
        print("ğŸ”„ æ›´æ–°ç°æœ‰å‘é‡æ•°æ®åº“")
        db = Chroma(
            persist_directory=chroma_dir,
            embedding_function=embeddings
        )
        db.add_documents(chunks)
    else:
        print("ğŸ†• åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“")
        Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            persist_directory=chroma_dir
        )
    
    print(f"âœ… å¤„ç†å®Œæˆï¼Œæ–°å¢ {len(chunks)} ä¸ªæ–‡æ¡£å—")

def parse_args() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="åˆ›å»ºæˆ–æ›´æ–°æ–‡æ¡£å‘é‡æ•°æ®åº“")
    parser.add_argument(
        "--datadir",
        type=str,
        default="data",
        help="æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼ˆé»˜è®¤ï¼š./dataï¼‰"
    )
    parser.add_argument(
        "--model",
        type=str,
        choices=["openai", "zhipuai"],
        default="openai",
        help="ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹æä¾›å•†ï¼ˆé»˜è®¤ï¼šopenaiï¼‰"
    )
    parser.add_argument(
        "--chromadir",
        type=str,
        default=DEFAULT_CHROMA_DIR,
        help=f"æŒ‡å®šChromaæ•°æ®åº“ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤ï¼š{DEFAULT_CHROMA_DIR}ï¼‰"
    )
    return parser.parse_args()

if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    
    try:
        create_or_update_vectorstore(args.datadir, args.model, args.chromadir)
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿé”™è¯¯: {str(e)}") 