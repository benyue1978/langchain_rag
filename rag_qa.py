import os
import warnings
import logging
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langsmith import Client
import datetime
import sys
from langchain.schema.runnable import RunnableLambda

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ç¡®ä¿è®¾ç½®äº†å¿…è¦çš„ç¯å¢ƒå˜é‡
assert os.getenv("OPENAI_API_KEY"), "è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡"
assert os.getenv("LANGSMITH_API_KEY"), "è¯·è®¾ç½® LANGSMITH_API_KEY ç¯å¢ƒå˜é‡"

# åˆå§‹åŒ– LangSmith å®¢æˆ·ç«¯
client = Client()

# é…ç½®æ—¥å¿—çº§åˆ«
logging.getLogger("pypdf").setLevel(logging.ERROR)

# è¿‡æ»¤è­¦å‘Šä¿¡æ¯
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

def init_qa_system(silent=False):
    """åˆå§‹åŒ–QAç³»ç»Ÿ
    
    Args:
        silent (bool): æ˜¯å¦é™é»˜åˆå§‹åŒ–ï¼ˆä¸æ˜¾ç¤ºæç¤ºä¿¡æ¯ï¼‰
        
    Returns:
        qa_chain: åˆå§‹åŒ–å¥½çš„QAé“¾
    """
    if not silent:
        print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ– QA ç³»ç»Ÿ...")
        
    # åˆå§‹åŒ–æ–‡æ¡£åŠ è½½å™¨å’Œåˆ†å‰²å™¨
    loader = PyPDFLoader("æ°®æ°§ä¼ æ„Ÿå™¨æ€§èƒ½åŠå…¶æ§åˆ¶ç­–ç•¥ç ”ç©¶-from SY.pdf")
    text_splitter = CharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100,
        length_function=len,
        is_separator_regex=False,
    )
    
    # åŠ è½½å’Œåˆ†å‰²æ–‡æ¡£
    pages = loader.load()
    documents = text_splitter.split_documents(pages)
    
    # åˆ›å»ºå‘é‡å­˜å‚¨
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(documents=documents, embedding=embeddings)
    
    # åˆ›å»ºQAé“¾
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )
    
    template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ°®æ°§ä¼ æ„Ÿå™¨ä¸“å®¶ã€‚ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜"ã€‚
    ä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚å°½é‡ä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚

    ä¸Šä¸‹æ–‡: {context}
    èŠå¤©å†å²: {chat_history}
    äººç±»: {question}

    åŠ©æ‰‹:"""
    
    QA_CHAIN_PROMPT = PromptTemplate(
        input_variables=["context", "chat_history", "question"],
        template=template,
    )
    
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        memory=memory,
        combine_docs_chain_kwargs={"prompt": QA_CHAIN_PROMPT},
        return_source_documents=True,
        return_generated_question=False,
        output_key="answer",
        verbose=False
    )
    
    if not silent:
        print("\nâœ¨ QA ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        
    return qa_chain

def create_evaluation_dataset():
    """åˆ›å»ºè¯„ä¼°æ•°æ®é›†"""
    dataset_name = f"nox_sensor_qa_eval_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
    dataset = client.create_dataset(dataset_name)
    
    examples = [
        {
            "inputs": {"question": "ä»€ä¹ˆæ˜¯æ°®æ°§ä¼ æ„Ÿå™¨ï¼Ÿ"},
            "outputs": {"answer": "æ°®æ°§ä¼ æ„Ÿå™¨æ˜¯ä¸€ç§ç”¨äºæ£€æµ‹å’Œæµ‹é‡æ°®æ°§åŒ–ç‰©æµ“åº¦çš„ä¼ æ„Ÿå™¨è£…ç½®ã€‚"}
        },
        {
            "inputs": {"question": "æ°®æ°§ä¼ æ„Ÿå™¨çš„ä¸»è¦åº”ç”¨åœºæ™¯æ˜¯ä»€ä¹ˆï¼Ÿ"},
            "outputs": {"answer": "ä¸»è¦åº”ç”¨äºæ±½è½¦å°¾æ°”æ’æ”¾æ§åˆ¶ç³»ç»Ÿï¼Œç”¨äºç›‘æµ‹å’Œæ§åˆ¶æ°®æ°§åŒ–ç‰©çš„æ’æ”¾ã€‚"}
        },
        {
            "inputs": {"question": "æ°®æ°§ä¼ æ„Ÿå™¨çš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ"},
            "outputs": {"answer": "åŸºäºç”µåŒ–å­¦åŸç†ï¼Œé€šè¿‡æµ‹é‡æ°§ç¦»å­çš„æµ“åº¦å·®æ¥æ£€æµ‹æ°®æ°§åŒ–ç‰©å«é‡ã€‚"}
        },
        {
            "inputs": {"question": "æ°®æ°§ä¼ æ„Ÿå™¨çš„æ€§èƒ½æŒ‡æ ‡æœ‰å“ªäº›ï¼Ÿ"},
            "outputs": {"answer": "ä¸»è¦åŒ…æ‹¬å“åº”æ—¶é—´ã€æµ‹é‡ç²¾åº¦ã€æ¸©åº¦ç‰¹æ€§å’Œä½¿ç”¨å¯¿å‘½ç­‰æŒ‡æ ‡ã€‚"}
        },
        {
            "inputs": {"question": "å¦‚ä½•æé«˜æ°®æ°§ä¼ æ„Ÿå™¨çš„æ§åˆ¶ç²¾åº¦ï¼Ÿ"},
            "outputs": {"answer": "é€šè¿‡ä¼˜åŒ–ä¼ æ„Ÿå™¨ç»“æ„ã€æ”¹è¿›æ§åˆ¶ç®—æ³•å’Œæé«˜ä¿¡å·å¤„ç†èƒ½åŠ›ç­‰æ–¹å¼ã€‚"}
        }
    ]
    
    client.create_examples(dataset_id=dataset.id, examples=examples)
    return dataset_name

def run_evaluation(qa_chain):
    """è¿è¡Œè¯„ä¼°æµ‹è¯•é›†"""
    dataset_name = create_evaluation_dataset()
    
    def construct_chain():
        input_mapper = RunnableLambda(
            lambda x: {"question": x["question"], "chat_history": []}
        )
        return input_mapper | qa_chain
    
    print("è¯„ä¼°ä¸­...")
    client.run_on_dataset(
        dataset_name=dataset_name,
        llm_or_chain_factory=construct_chain,
        project_metadata={"tags": ["nox_sensor_qa_eval"]},
        verbose=False
    )
    print("å®Œæˆ")

def interactive_mode(qa_chain):
    """äº¤äº’å¼é—®ç­”æ¨¡å¼"""
    print("\nğŸ’¬ å¼€å§‹äº¤äº’å¼é—®ç­”...\n")
    
    while True:
        question = input("â“ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºï¼‰: ").strip()
        
        if question.lower() in ['quit', 'exit']:
            print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼å†è§ï¼")
            break
            
        if not question:
            print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜ï¼")
            continue
            
        try:
            result = qa_chain.invoke({"question": question})
            answer = result['answer']
            source_docs = result['source_documents']
            
            print("\nğŸ’¡ ç­”æ¡ˆ:", answer)
            
            if source_docs:
                print("\nğŸ“š å‚è€ƒæ¥æº:")
                for i, doc in enumerate(source_docs[:2], 1):
                    print(f"{i}. {doc.page_content[:150]}...")
            
            print("\n" + "-"*50 + "\n")
            
        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
            print("è¯·ç¨åé‡è¯•æˆ–è”ç³»ç³»ç»Ÿç®¡ç†å‘˜ã€‚\n")

def main():
    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©æ¨¡å¼
    is_eval_mode = len(sys.argv) > 1 and sys.argv[1] == "--evaluate"
    
    # åˆå§‹åŒ–QAç³»ç»Ÿ
    qa_chain = init_qa_system(silent=is_eval_mode)
    
    # è¿è¡Œç›¸åº”çš„æ¨¡å¼
    if is_eval_mode:
        run_evaluation(qa_chain)
    else:
        interactive_mode(qa_chain)

if __name__ == "__main__":
    main() 