import os
from pathlib import Path
from typing import List, Dict, Any
from langchain_community.document_loaders import UnstructuredPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ç³»ç»Ÿé…ç½®
CHROMA_DIR = "chroma_db_hardware"
PDF_FILES = ["data/r01uh0368ej0220_rl78f13_hardware.pdf", "data/r01us0015ej0230-rl78-software.pdf"]

# æ¨¡å‹é…ç½®
EMBEDDING_MODEL = "text-embedding-ada-002"  # OpenAI åµŒå…¥æ¨¡å‹
CHAT_MODEL = "gpt-3.5-turbo"  # OpenAI å¯¹è¯æ¨¡å‹
TEMPERATURE = 0  # æ¸©åº¦å‚æ•°ï¼š0è¡¨ç¤ºæœ€ç¡®å®šæ€§çš„å›ç­”ï¼Œ1è¡¨ç¤ºæœ€å…·åˆ›é€ æ€§
CHUNK_SIZE = 500  # æ–‡æ¡£åˆ†å—å¤§å°
CHUNK_OVERLAP = 50  # åˆ†å—é‡å å¤§å°
TOP_K_RESULTS = 5  # æ£€ç´¢æ—¶è¿”å›çš„æœ€ç›¸å…³æ–‡æ¡£æ•°é‡

def load_and_process_documents(pdf_paths: List[str]) -> List[Any]:
    """åŠ è½½å¹¶å¤„ç†PDFæ–‡æ¡£
    
    Args:
        pdf_paths: PDFæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        
    Returns:
        åŠ è½½çš„æ–‡æ¡£åˆ—è¡¨
    """
    all_docs = []
    for path in pdf_paths:
        try:
            loader = UnstructuredPDFLoader(path)
            docs = loader.load()
            print(f"âœ… æˆåŠŸåŠ è½½æ–‡æ¡£: {path}")
            all_docs.extend(docs)
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥ {path}: {str(e)}")
    return all_docs

def split_documents(documents: List[Any]) -> List[Any]:
    """åˆ†å‰²æ–‡æ¡£ä¸ºè¾ƒå°çš„å—
    
    Args:
        documents: è¦åˆ†å‰²çš„æ–‡æ¡£åˆ—è¡¨
        
    Returns:
        åˆ†å‰²åçš„æ–‡æ¡£å—åˆ—è¡¨
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?"]  # ä¼˜åŒ–ä¸­æ–‡åˆ†å‰²
    )
    return splitter.split_documents(documents)

def init_vectorstore() -> Chroma:
    """åˆå§‹åŒ–å‘é‡å­˜å‚¨
    
    ä½¿ç”¨ OpenAI çš„ text-embedding-ada-002 æ¨¡å‹ç”Ÿæˆæ–‡æ¡£åµŒå…¥ã€‚
    é¦–æ¬¡è¿è¡Œæ—¶ä¼šåˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨ï¼Œåç»­è¿è¡Œä¼šç›´æ¥åŠ è½½å·²å­˜åœ¨çš„å­˜å‚¨ã€‚
    
    Returns:
        Chromaå‘é‡å­˜å‚¨å®ä¾‹
    """
    embeddings = OpenAIEmbeddings(
        model=EMBEDDING_MODEL,
        show_progress_bar=True
    )
    
    if Path(CHROMA_DIR).exists():
        print(f"ğŸŸ¡ åŠ è½½å·²å­˜åœ¨çš„ Chroma DB: {CHROMA_DIR}")
        return Chroma(
            persist_directory=CHROMA_DIR,
            embedding_function=embeddings
        )

    print(f"ğŸŸ¢ åˆå§‹åŒ–æ–°çš„ Chroma DB: {CHROMA_DIR}")
    documents = load_and_process_documents(PDF_FILES)
    chunks = split_documents(documents)
    db = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=CHROMA_DIR
    )
    print(f"âœ… å‘é‡å­˜å‚¨åˆå§‹åŒ–å®Œæˆï¼Œå…±å¤„ç† {len(chunks)} ä¸ªæ–‡æ¡£å—")
    return db

def run_qa_interface(vectorstore: Chroma) -> None:
    """è¿è¡Œé—®ç­”æ¥å£
    
    ä½¿ç”¨ ChatGPT (gpt-3.5-turbo) æ¨¡å‹å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼Œ
    é€šè¿‡å‘é‡ç›¸ä¼¼åº¦æœç´¢æ‰¾åˆ°ç›¸å…³æ–‡æ¡£å†…å®¹ã€‚
    
    Args:
        vectorstore: å‘é‡å­˜å‚¨å®ä¾‹
    """
    retriever = vectorstore.as_retriever(
        search_kwargs={"k": TOP_K_RESULTS}
    )
    
    llm = ChatOpenAI(
        model=CHAT_MODEL,
        temperature=TEMPERATURE,
        streaming=True  # å¯ç”¨æµå¼è¾“å‡º
    )
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
    )

    print("\nğŸ¤– ç¡¬ä»¶æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")
    print("- è¾“å…¥é—®é¢˜å¹¶æŒ‰å›è½¦")
    print("- è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º")
    print("- è¾“å…¥ 'help' è·å–å¸®åŠ©")
    
    while True:
        query = input("\n> ").strip()
        
        if not query:
            continue
            
        if query.lower() in ["exit", "quit"]:
            print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼")
            break
            
        if query.lower() == "help":
            print("\nğŸ“– å¸®åŠ©ä¿¡æ¯:")
            print("- æ‚¨å¯ä»¥è¯¢é—®æœ‰å…³ç¡¬ä»¶çš„ä»»ä½•é—®é¢˜")
            print("- ç³»ç»Ÿä¼šä»æ–‡æ¡£ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯å¹¶ç”Ÿæˆå›ç­”")
            print("- æ¯ä¸ªå›ç­”éƒ½ä¼šæ˜¾ç¤ºä¿¡æ¯æ¥æº")
            continue
            
        try:
            result = qa_chain.invoke({"query": query})
            print(f"\nğŸ§  å›ç­”:\n{result['result']}")
            print("\nğŸ“„ å‚è€ƒæ¥æº:")
            for doc in result["source_documents"]:
                print(f" - {doc.metadata.get('source', 'æœªçŸ¥æ¥æº')}")
        except Exception as e:
            print(f"âŒ å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {str(e)}")

if __name__ == "__main__":
    if not os.getenv("OPENAI_API_KEY"):
        api_key = input("è¯·è¾“å…¥æ‚¨çš„OpenAI APIå¯†é’¥: ").strip()
        os.environ["OPENAI_API_KEY"] = api_key
        
    try:
        vectorstore = init_vectorstore()
        run_qa_interface(vectorstore)
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿé”™è¯¯: {str(e)}")